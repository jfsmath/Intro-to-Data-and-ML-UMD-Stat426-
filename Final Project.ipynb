{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL PROJECT\n",
    "\n",
    "In this final project, we compare the tools that we have learned to classify the Fashion MNIST data set. You can read about the description of the data set here:\n",
    "\n",
    "https://github.com/zalandoresearch/fashion-mnist\n",
    "\n",
    "This porject is roughly divided into four parts:\n",
    "* Use PCA to reduce the dimension of each image. \n",
    "* KNN on PCA\n",
    "* KNN on LDA\n",
    "* Neural network / convolution neural network using keras\n",
    "\n",
    "If you are interested, you can try other learing methods that we talked about / methods that you know but we did not cover in the class. \n",
    "\n",
    "* For each method, I want you to write one to two paragraphs explaining the fundamental ideas about the learning methods. For example, explain the mechanism of the algorithm, what the key parameters are and how to choose them, and pros and cons of the methods.\n",
    "\n",
    "* When you see a question, you can create a Markdown below the question block (or the code block, whichever makes more sense) and write your answer in it\n",
    "\n",
    "* For the write up, please explain as much of your code as possible, and avoid a large block of code (try to put them in different blocks). Keep all of the intermediate plots if any. \n",
    "\n",
    "* Set the random seed to 42 for reproducibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If any of the modules above is missing, you can use the following command to install it:\n",
    "\n",
    "python3 -m pip install MODULE NAME\n",
    "\n",
    "If you like, you are also welcome to use PyTorch for the neural network part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess the data\n",
    "train_images = train_images.reshape((-1, 28 * 28)).astype('float32') / 255.0\n",
    "test_images = test_images.reshape((-1, 28 * 28)).astype('float32') / 255.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets using a 80/20 ratio\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, \n",
    "                                                                      test_size=0.2, \n",
    "                                                                      random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the training/validation data\n",
    "scaler = StandardScaler()\n",
    "train_images_scaled = scaler.fit_transform(train_images)\n",
    "val_images_scaled = scaler.transform(val_images)\n",
    "test_images_scaled = scaler.transform(test_images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Principal Component Analysis\n",
    "\n",
    "A question to ask ourselves: Should the data really live in 784 dimensions?\n",
    "* The following block of code estimates the number of components to account for 90% of the variance in the data. What is the number of components needed?\n",
    "* Modify the code, and plot the number of components against the explained variance for the range 60-95% with 1% increment in each iteration\n",
    "* To speed things up, you may use the first 1000 of the training images instead of all of them. This applies to the rest of the PCA section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA, initialed with unknown number of components\n",
    "pca = PCA(n_components=None)  \n",
    "\n",
    "# Fit PCA to training image\n",
    "pca.fit(train_images_scaled)\n",
    "\n",
    "# Calculate the cumulative explained variance ratio\n",
    "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Find the number of components required to explain 90% of the variance\n",
    "n_components = np.argmax(cumulative_variance_ratio >= 0.90) + 1\n",
    "\n",
    "pca = PCA(n_components=n_components)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can now apply the updated PCA to the data\n",
    "# (depending on the number of components you end up using)\n",
    "\n",
    "train_images_pca = pca.fit_transform(train_images_scaled)\n",
    "val_images_pca = pca.transform(val_images_scaled)\n",
    "test_images_pca = pca.transform(test_images_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What was the advantage of this projection into the reduced dimensional space? We can\n",
    "expect the algorithms will run MUCH faster on the reduced dimension data, but will we\n",
    "sacrifice accuracy for this speed boost? We investiage this in the following part. \n",
    "\n",
    "\n",
    "## PART 2 k-Nearest Neighbor\n",
    "\n",
    "Task:\n",
    "* Give a description of the mechanism of the kNN algorithm\n",
    "* Run the kNN on both the original data AND on the reduced dimension data from PCA (90% total variance explained on the training set)\n",
    "* Consider k = [1;3;5;7;9;11]. Use 10-fold Cross-Validation to tune choose the best k\n",
    "    - Please include a runing time evaluation\n",
    "    - Compare the performance across data sets both based on accuracy (on the test data)\n",
    "and on running time\n",
    "    - Name and save the KNN model (for the best tuned k) re-trained on the full PCA\n",
    "training set, as knn_best (you will need this for later comparisons).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some potentially useful code for kNN, \n",
    "# replace X_train/X_test/y_train/y_test with actual data\n",
    "\n",
    "\n",
    "# Initialize the KNN classifier with 1-nearest neighbor\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test data\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Perform 10-fold cross-validation on the original data set (X,y)\n",
    "scores = cross_val_score(knn, X, y, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: LDA/QDA and SVM\n",
    "\n",
    "We will now consider other classifiers trained on the PCA dataset.  \n",
    "\n",
    "Tasks:\n",
    "* Use the SVM to train\n",
    "    - one using linear kernel\n",
    "    - one using RBF\n",
    "* Use the sklearn.discriminant_analysis to train\n",
    "    - one LDA model\n",
    "    - one QDA model\n",
    "* For each model (including the kNN), compare test accuracy for the tuned model as well as runtimes\n",
    "* Based on the time and accuracy, which model would you choose out of these?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The basic building blocks for SVM\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "clf = svm.SVC(kernel='linear')  # Linear kernel\n",
    "clf = svm.SVC(kernel='rbf') # RBF kernel\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building block for LDA\n",
    "\n",
    "# Initialize the LDA classifier\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Fit the LDA classifier to the training data\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test data using LDA\n",
    "y_pred_lda = lda.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the LDA model\n",
    "accuracy_lda = accuracy_score(y_test, y_pred_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building block for QDA\n",
    "\n",
    "# Initialize the QDA classifier\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "# Fit the QDA classifier to the training data\n",
    "qda.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test data using QDA\n",
    "y_pred_qda = qda.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the QDA model\n",
    "accuracy_qda = accuracy_score(y_test, y_pred_qda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: (Deep) Neural Network\n",
    "\n",
    "This section heavily relies on keras (if you are sticking with tensorflow).\n",
    "\n",
    "Tasks:\n",
    "* You can use the same CNN architecture considered for the MNIST dataset discussed in class (or feel free to try other\n",
    "architectures). \n",
    "* Train using categorical cross entropy loss, adam optimizer, and track the\n",
    "training and validation accuracy. \n",
    "* Train for 30 epochs, batchsize of 128, and a validation split of 0.2. Remember to time the training.\n",
    "* In your write-up, include the history plots for training and validation sets. How much does the test accuracy improve by compared to the previous classification methods?\n",
    "* Re-train the DNN but on the training set with only 1000 samples (remember to extend the channel dimension for x to use 2D Conv layers). Does the DNN still yield better performance than the previous classifiers with fewer training samples? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some building blocks for CNN\n",
    "\n",
    "# Reshape data to fit the model\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_test = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=30, \n",
    "                    batch_size=128, validation_data=(X_test, y_test), \n",
    "                    verbose=0)\n",
    "\n",
    "# Plot training error over epochs\n",
    "plt.plot(history.history['loss'], label='Training Error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Training Error Over 30 Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
